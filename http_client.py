import asyncio
import uuid
import aiohttp
import logging
from typing import Dict, Any, Optional, Union, Callable, Awaitable, AsyncGenerator
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime
import time
from apscheduler.schedulers.asyncio import AsyncIOScheduler
import orjson

# ==========================================
# PART 1: AsyncHttpClient å®ç° (å¤åˆ¶è‡ªä¸Šä¸€è½®æœ€ç»ˆç‰ˆæœ¬)
# ==========================================

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- ç±»å‹å®šä¹‰ ---
class CancelBehavior(Enum):
    DO_NOTHING = 0
    TRIGGER_SUCCESS = 1
    TRIGGER_FAILURE = 2

class RequestStatus(Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

@dataclass
class RequestResult:
    request_id: str
    status: RequestStatus
    content: Union[str, bytes, None]
    http_code: Optional[int] = None
    error: Optional[Exception] = None

# å›è°ƒå‡½æ•°ç­¾å
OnStreamStartCallback = Callable[[str, float, Any], Union[None, Awaitable[None]]]
CallbackType = Callable[['RequestResult', Any], Union[None, Awaitable[None]]]

@dataclass
class RequestWrapper:
    url: str
    method: str = "GET"
    params: Optional[Dict[str, Any]] = None
    headers: Optional[Dict[str, str]] = None
    data: Any = None
    json: Any = None
    
    user_data: Any = None 
    
    # å›è°ƒé…ç½®
    on_stream_start: Optional[OnStreamStartCallback] = None
    stream_start_data: Any = None 

    keep_content_in_memory: bool = True 
    max_retries: int = 0
    retry_interval: float = 1.0
    is_stream: bool = False
    retry_on_stream_error: bool = True 
    timeout: int = 60
    cancel_behavior: CancelBehavior = CancelBehavior.TRIGGER_FAILURE
    on_success: Optional[CallbackType] = None
    on_failure: Optional[CallbackType] = None
    extra_options: Dict[str, Any] = field(default_factory=dict)
    def __post_init__(self): self.method = self.method.upper()

class HttpErrorWithContent(Exception):
    def __init__(self, status_code, content, message):
        super().__init__(message)
        self.status_code = status_code
        self.content = content

class RequestContext:
    def __init__(self, request_id: str, wrapper: RequestWrapper):
        self.request_id = request_id
        self.wrapper = wrapper
        self.task: Optional[asyncio.Task] = None
        self.status = RequestStatus.PENDING
        self.created_at = datetime.now()
        self.stream_queue: Optional[asyncio.Queue] = asyncio.Queue(maxsize=1000) if wrapper.is_stream else None
        self.sentinel = object()
        # ç”¨äº server.py ç­‰å¾…è¿æ¥å»ºç«‹ç»“æœ
        self.startup_future: asyncio.Future = asyncio.Future()

class AsyncHttpClient:
    def __init__(self):
        self.active_requests: Dict[str, RequestContext] = {}
        self.session: Optional[aiohttp.ClientSession] = None
        self.scheduler = AsyncIOScheduler()
        self.scheduler.add_job(self._garbage_collector, 'interval', seconds=60)
        self.scheduler.start()

    async def get_session(self) -> aiohttp.ClientSession:
        if self.session is None or self.session.closed:
            self.session = aiohttp.ClientSession()
        return self.session

    def submit(self, request: RequestWrapper) -> str:
        req_id = str(uuid.uuid4())
        ctx = RequestContext(req_id, request)
        task = asyncio.create_task(self._worker(ctx))
        ctx.task = task
        self.active_requests[req_id] = ctx
        return req_id

    async def wait_for_upstream_status(self, request_id: str):
        """ç­‰å¾…ä¸Šæ¸¸è¿æ¥å»ºç«‹ç»“æœï¼Œå¦‚æœé‡è¯•ä¸­ï¼Œè¿™é‡Œä¼šä¸€ç›´æŒ‚èµ·ï¼Œç›´åˆ°é‡è¯•æˆåŠŸæˆ–å½»åº•å¤±è´¥"""
        ctx = self.active_requests.get(request_id)
        if not ctx: raise ValueError("Invalid Request ID")
        await ctx.startup_future

    async def _worker(self, ctx: RequestContext):
        req = ctx.wrapper
        ctx.status = RequestStatus.RUNNING
        session = await self.get_session()
        
        request_kwargs = {
            'params': req.params,
            'headers': req.headers or {},
            'timeout': aiohttp.ClientTimeout(total=req.timeout),
            **req.extra_options
        }

        if req.json is not None:
            request_kwargs['data'] = orjson.dumps(req.json)
            if 'headers' not in request_kwargs: request_kwargs['headers'] = {}
            request_kwargs['headers']['Content-Type'] = 'application/json'
        elif req.data is not None:
            request_kwargs['data'] = req.data

        total_attempts = req.max_retries + 1
        accumulated_body = bytearray()
        last_http_code, last_error = None, None
        
        for attempt in range(1, total_attempts + 1):
            accumulated_body = bytearray()
            last_http_code, last_error = None, None
            stream_started_successfully = False
            start_time = time.perf_counter()

            try:
                if attempt > 1:
                    logger.info(f"ğŸ”„ Retry attempt {attempt}/{total_attempts} for {ctx.request_id}")

                async with session.request(req.method, req.url, **request_kwargs) as response:
                    last_http_code = response.status
                    
                    # --- 1. é”™è¯¯çŠ¶æ€ç å¤„ç† ---
                    if response.status >= 400:
                        try:
                            error_bytes = await response.read()
                            if req.keep_content_in_memory: accumulated_body.extend(error_bytes)
                        except: error_bytes = b""
                        
                        exc = HttpErrorWithContent(response.status, error_bytes, f"HTTP {response.status}")
                        
                        # ã€ä¿®å¤ç‚¹ 1ã€‘: ä¸è¦åœ¨è¿™é‡Œ set_exceptionã€‚
                        # å¦‚æœè®¾ç½®äº†ï¼Œserver.py ä¼šç«‹å³æ”¶åˆ°é”™è¯¯å¹¶åœæ­¢ç­‰å¾…ï¼Œå¯¼è‡´é‡è¯•æ— æ•ˆã€‚
                        # æˆ‘ä»¬åªæŠ›å‡ºå¼‚å¸¸ï¼Œè®©ä¸‹é¢çš„ except å—å¤„ç†é‡è¯•é€»è¾‘ã€‚
                        raise exc

                    # --- 2. è¿æ¥æˆåŠŸ (200 OK) ---
                    # åªæœ‰åœ¨è¿™é‡ŒæˆåŠŸäº†ï¼Œæ‰é€šçŸ¥ Future æˆåŠŸ
                    if not ctx.startup_future.done():
                        ctx.startup_future.set_result(response.status)

                    if req.is_stream:
                        stream_started_successfully = True
                        is_first_chunk = True
                        async for chunk in response.content.iter_any():
                            # é¦–å­—å›è°ƒ
                            if is_first_chunk:
                                ttft = time.perf_counter() - start_time
                                self._trigger_stream_start_callback(req.on_stream_start, ctx.request_id, ttft, req.stream_start_data)
                                is_first_chunk = False

                            await ctx.stream_queue.put(chunk)
                            if req.keep_content_in_memory: accumulated_body.extend(chunk)
                        await ctx.stream_queue.put(ctx.sentinel)
                    else:
                        if req.keep_content_in_memory:
                            body_bytes = await response.read()
                            accumulated_body.extend(body_bytes)
                        else: await response.read()

                    ctx.status = RequestStatus.COMPLETED
                    final_content = self._prepare_content(accumulated_body, req.keep_content_in_memory)
                    self._trigger_callback(req.on_success, RequestResult(ctx.request_id, RequestStatus.COMPLETED, final_content, response.status), req.user_data)
                    return 

            except asyncio.CancelledError as e:
                ctx.status = RequestStatus.CANCELLED
                partial = self._prepare_content(accumulated_body, req.keep_content_in_memory)
                if req.is_stream: await ctx.stream_queue.put(e)
                
                # å–æ¶ˆæ˜¯ä¸å¯æ¢å¤çš„ï¼Œå¿…é¡»ç«‹å³é€šçŸ¥ Future
                if not ctx.startup_future.done(): ctx.startup_future.set_exception(e)

                res = RequestResult(ctx.request_id, RequestStatus.CANCELLED, partial, last_http_code, e)
                if req.cancel_behavior == CancelBehavior.TRIGGER_SUCCESS: self._trigger_callback(req.on_success, res, req.user_data)
                elif req.cancel_behavior == CancelBehavior.TRIGGER_FAILURE: self._trigger_callback(req.on_failure, res, req.user_data)
                raise

            except Exception as e:
                last_error = e
                if isinstance(e, HttpErrorWithContent): last_http_code = e.status_code
                
                # ç‰¹æ®Šæƒ…å†µï¼šæµå·²ç»å¼€å§‹ä½†ä¸å…è®¸æµé”™è¯¯é‡è¯• -> ç«‹å³å¤±è´¥
                if req.is_stream and stream_started_successfully and not req.retry_on_stream_error:
                    logger.warning(f"Stream interrupted, no retry: {e}")
                    # ã€ä¿®å¤ç‚¹ 2ã€‘: å†³å®šä¸å†é‡è¯•äº†ï¼Œæ‰é€šçŸ¥ Future å¤±è´¥
                    if not ctx.startup_future.done(): ctx.startup_future.set_exception(e)
                    break 
                
                # æ£€æŸ¥é‡è¯•æ¬¡æ•°
                if attempt < total_attempts:
                    logger.warning(f"Attempt {attempt} failed: {e}. Retrying in {req.retry_interval}s...")
                    # æ­£åœ¨é‡è¯•ä¸­... Future ä¿æŒ Pending çŠ¶æ€ï¼Œserver.py ç»§ç»­ç­‰å¾…
                    await asyncio.sleep(req.retry_interval)
                    continue 
                else: 
                    # ã€ä¿®å¤ç‚¹ 3ã€‘: é‡è¯•æ¬¡æ•°è€—å°½ï¼Œå½»åº•å¤±è´¥ï¼Œé€šçŸ¥ Future
                    logger.error(f"All attempts failed: {e}")
                    if not ctx.startup_future.done(): ctx.startup_future.set_exception(e)
                    break
        
        ctx.status = RequestStatus.FAILED
        if req.is_stream:
            error_to_propagate = last_error if last_error else Exception("Unknown Error in Worker")
            await ctx.stream_queue.put(error_to_propagate)

        # å…œåº•
        if not ctx.startup_future.done():
            ctx.startup_future.set_exception(last_error or Exception("Worker Failed"))

        final_content = self._prepare_content(accumulated_body, req.keep_content_in_memory)
        self._trigger_callback(req.on_failure, RequestResult(ctx.request_id, RequestStatus.FAILED, final_content, last_http_code, last_error), req.user_data)

    def _prepare_content(self, data: bytearray, keep_memory: bool) -> Union[str, bytes]:
        if not keep_memory: return "[Content Dropped]"
        try: return data.decode('utf-8')
        except: return bytes(data)

    def _trigger_stream_start_callback(self, callback: OnStreamStartCallback, req_id: str, ttft: float, custom_data: Any):
        if not callback: return
        if asyncio.iscoroutinefunction(callback): asyncio.create_task(callback(req_id, ttft, custom_data))
        else: asyncio.get_running_loop().run_in_executor(None, self._run_sync_stream_start_callback, callback, req_id, ttft, custom_data)

    def _run_sync_stream_start_callback(self, callback, req_id, ttft, custom_data):
        try: callback(req_id, ttft, custom_data)
        except Exception: pass

    def _trigger_callback(self, callback: CallbackType, result: RequestResult, user_data: Any):
        if not callback: return
        if asyncio.iscoroutinefunction(callback): asyncio.create_task(callback(result, user_data))
        else: asyncio.get_running_loop().run_in_executor(None, self._run_sync_callback_safely, callback, result, user_data)

    def _run_sync_callback_safely(self, callback, result, user_data):
        try: callback(result, user_data)
        except Exception: pass

    async def cancel_request(self, request_id: str):
        ctx = self.active_requests.get(request_id)
        if ctx and ctx.task and not ctx.task.done():
            ctx.task.cancel()
            return True
        return False

    async def stream_generator(self, request_id: str) -> AsyncGenerator[bytes, None]:
        ctx = self.active_requests.get(request_id)
        if not ctx or not ctx.wrapper.is_stream: raise ValueError("Invalid ID")
        while True:
            data = await ctx.stream_queue.get()
            if isinstance(data, Exception): raise data
            if data is ctx.sentinel: break
            yield data

    async def _garbage_collector(self):
        for k in [k for k, v in self.active_requests.items() if v.task.done()]: del self.active_requests[k]

    async def close(self):
        if self.session: await self.session.close()
        self.scheduler.shutdown()
