import asyncio
import aiohttp
import uuid
import logging
import orjson
import time
from enum import Enum
from typing import Callable, Optional, Dict, Any, AsyncGenerator, Union, Awaitable
from dataclasses import dataclass, field
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from datetime import datetime, timedelta

# ÈÖçÁΩÆÊó•Âøó
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Êûö‰∏æ‰∏éÊï∞ÊçÆÁ±ª ---

class CancelBehavior(Enum):
    DO_NOTHING = 0
    TRIGGER_SUCCESS = 1
    TRIGGER_FAILURE = 2

class RequestStatus(Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

@dataclass
class RequestResult:
    request_id: str
    status: RequestStatus
    content: Union[str, bytes, None]
    http_code: Optional[int] = None
    error: Optional[Exception] = None
    json_body: Any = None 

# ÂõûË∞ÉÂáΩÊï∞Á±ªÂûãÂÆö‰πâ
OnStreamStartCallback = Callable[[str, float, Any], Union[None, Awaitable[None]]]
CallbackType = Callable[['RequestResult', Any], Union[None, Awaitable[None]]]

@dataclass
class RequestWrapper:
    """
    HTTP ËØ∑Ê±ÇÂ∞ÅË£ÖÁ±ª
    """
    url: str
    method: str = "GET"
    params: Optional[Dict[str, Any]] = None
    headers: Optional[Dict[str, str]] = None
    data: Any = None
    json: Any = None
    
    # ÂõûË∞ÉÁõ∏ÂÖ≥
    user_data: Any = None 
    on_stream_start: Optional[OnStreamStartCallback] = None
    stream_start_data: Any = None 
    on_success: Optional[CallbackType] = None
    on_failure: Optional[CallbackType] = None
    
    # Ë°å‰∏∫ÈÖçÁΩÆ
    keep_content_in_memory: bool = True 
    max_memory_size: int = 100 * 1024 * 1024 # 100 MB

    max_retries: int = 0
    retry_interval: float = 1.0
    is_stream: bool = False
    retry_on_stream_error: bool = True 
    timeout: int = 60
    cancel_behavior: CancelBehavior = CancelBehavior.TRIGGER_FAILURE
    
    extra_options: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self): 
        self.method = self.method.upper()

class HttpErrorWithContent(Exception):
    """Êê∫Â∏¶‰∏äÊ∏∏ËøîÂõûÁöÑÈîôËØØ Body ÁöÑÂºÇÂ∏∏"""
    def __init__(self, status_code: int = 500, content: Union[str, bytes, None] = None, message: str = ""):
        super().__init__(message)
        self.status_code = status_code
        self.content = content

class RequestContext:
    def __init__(self, request_id: str, wrapper: RequestWrapper):
        self.request_id = request_id
        self.wrapper = wrapper
        self.task: Optional[asyncio.Task] = None
        self.status = RequestStatus.PENDING
        self.created_at = datetime.now()
        self.finished_at: Optional[datetime] = None # Áî®‰∫é TTL GC
        
        self.stream_queue: Optional[asyncio.Queue] = asyncio.Queue(maxsize=1000) if wrapper.is_stream else None

        # [Êñ∞Â¢û] Ê†áËÆ∞Ê∂àË¥πËÄÖÊòØÂê¶Â∑≤Êñ≠ÂºÄ
        self.consumer_disconnected: bool = False

        self.sentinel = object()
        
        # ‰∏§‰∏™ Future Áî®‰∫éÂêåÊ≠•Áä∂ÊÄÅ
        self.startup_future: asyncio.Future = asyncio.Future() # ËøûÊé•Âª∫Á´ãÔºàHeaderÊé•Êî∂Ôºâ
        self.result_future: asyncio.Future = asyncio.Future()  # ËØ∑Ê±ÇÂΩªÂ∫ïÂÆåÊàê

# --- Ê†∏ÂøÉÂÆ¢Êà∑Á´Ø ---

class AsyncHttpClient:
    def __init__(self, result_retention_seconds: int = 300):
        self.active_requests: Dict[str, RequestContext] = {}
        self.session: Optional[aiohttp.ClientSession] = None
        self.result_retention_seconds = result_retention_seconds
        
        self.scheduler = AsyncIOScheduler()
        self.scheduler.add_job(self._garbage_collector, 'interval', seconds=60)
        self.scheduler.start()

    async def get_session(self) -> aiohttp.ClientSession:
        if self.session is None or self.session.closed:
            # ‰∏çËÆæÁΩÆ json_serializeÔºåÊàë‰ª¨Âú® _worker ‰∏≠ÊâãÂä®Áî® orjson
            self.session = aiohttp.ClientSession()
        return self.session

    def submit(self, request: RequestWrapper) -> str:
        req_id = str(uuid.uuid4())
        ctx = RequestContext(req_id, request)
        task = asyncio.create_task(self._worker(ctx))
        ctx.task = task
        self.active_requests[req_id] = ctx
        return req_id
    
    def is_alive(self, request_id: str) -> bool:
        """
        Ê£ÄÊü•ËØ∑Ê±ÇÊòØÂê¶‰æùÁÑ∂Ê¥ªË∑ÉÔºàPending Êàñ RunningÔºâ„ÄÇ
        Â¶ÇÊûúËØ∑Ê±ÇÂ∑≤ÂÆåÊàê„ÄÅÂ§±Ë¥•„ÄÅÂèñÊ∂àÊàñÂ∑≤Ë¢´ GC Ê∏ÖÁêÜÔºåËøîÂõû False„ÄÇ
        """
        ctx = self.active_requests.get(request_id)
        
        # 1. Â¶ÇÊûú‰∏ä‰∏ãÊñá‰∏çÂ≠òÂú®ÔºàÂ∑≤Ë¢´ GC Ê∏ÖÁêÜÔºâÔºåÂàô‰∏çÊ¥ªË∑É
        if not ctx:
            return False
        
        # 2. Â¶ÇÊûú‰ªªÂä°ËøòÊ≤°ÂàõÂª∫ÔºàÂ§Ñ‰∫é PendingÔºâÔºåËßÜ‰∏∫Ê¥ªË∑É
        if ctx.task is None:
            return True
        
        # 3. Ê£ÄÊü• asyncio.Task ÊòØÂê¶ÂÆåÊàê
        if ctx.task.done():
            return False
            
        return True

    async def wait_for_upstream_status(self, request_id: str):
        """Á≠âÂæÖ‰∏äÊ∏∏ËøûÊé•Âª∫Á´ãÁªìÊûúÔºåÈÅáÂà∞ 400/500 ‰ºöÊäõÂá∫ÂºÇÂ∏∏"""
        ctx = self.active_requests.get(request_id)
        if not ctx: raise ValueError("Invalid Request ID")
        await ctx.startup_future

    async def _get_final_result(self, request_id: str) -> RequestResult:
        ctx = self.active_requests.get(request_id)
        if not ctx: raise ValueError(f"Request {request_id} not found or cleaned up")
        return await ctx.result_future

    async def content(self, request_id: str) -> bytes:
        res = await self._get_final_result(request_id)
        if isinstance(res.content, str): return res.content.encode('utf-8')
        return res.content or b""

    async def text(self, request_id: str, encoding: str = 'utf-8', errors: str = 'replace') -> str:
        res = await self._get_final_result(request_id)
        if isinstance(res.content, str): return res.content
        if isinstance(res.content, bytes):
            return res.content.decode(encoding, errors)
        if res.content is None:
            return ""
        return res.content.decode(encoding, errors) if res.content else ""

    async def json(self, request_id: str) -> Any:
        res = await self._get_final_result(request_id)
        if res.json_body is not None: return res.json_body
        if res.content: return orjson.loads(res.content)
        return None

    def _try_parse_json(self, data: bytearray) -> Any:
        if not data: return None
        try: return orjson.loads(bytes(data))
        except Exception: return None

    async def _worker(self, ctx: RequestContext):
        req = ctx.wrapper
        ctx.status = RequestStatus.RUNNING
        session = await self.get_session()
        deadline = time.time() + req.timeout
        
        request_kwargs = {
            'params': req.params, 'headers': req.headers or {}, **req.extra_options
        }
        if req.json is not None:
            request_kwargs['data'] = orjson.dumps(req.json)
            if 'headers' not in request_kwargs: request_kwargs['headers'] = {}
            request_kwargs['headers']['Content-Type'] = 'application/json'
        elif req.data is not None:
            request_kwargs['data'] = req.data

        total_attempts = req.max_retries + 1
        accumulated_body = bytearray()
        last_http_code, last_error = None, None
        
        try:
            for attempt in range(1, total_attempts + 1):
                remaining_time = deadline - time.time()
                if remaining_time <= 0:
                    last_error = asyncio.TimeoutError("Global timeout exceeded")
                    logger.error(f"‚ùå Global timeout exceeded for {ctx.request_id}")
                    break
                
                request_kwargs['timeout'] = aiohttp.ClientTimeout(total=remaining_time)

                accumulated_body = bytearray()
                last_http_code, last_error = None, None
                stream_started_successfully = False
                start_time = time.perf_counter()

                try:
                    if attempt > 1:
                        logger.info(f"üîÑ Retry attempt {attempt}/{total_attempts} for {ctx.request_id}")

                    async with session.request(req.method, req.url, **request_kwargs) as response:
                        last_http_code = response.status
                        
                        # --- ÈîôËØØÁä∂ÊÄÅÁ†ÅÂ§ÑÁêÜ ---
                        if response.status >= 400:
                            try:
                                error_bytes = await response.read()
                                if req.keep_content_in_memory: accumulated_body.extend(error_bytes)
                            except: error_bytes = b""
                            
                            # ÊäõÂá∫ÂºÇÂ∏∏ËøõÂÖ• except ÂùóÂ§ÑÁêÜÔºàÂÜ≥ÂÆöÊòØÂê¶ÈáçËØïÔºâ
                            raise HttpErrorWithContent(response.status, error_bytes, f"HTTP {response.status}")

                        # --- ËøûÊé•ÊàêÂäüÔºåÈÄöÁü• wait_for_upstream_status ---
                        if not ctx.startup_future.done():
                            ctx.startup_future.set_result(response.status)

                        if req.is_stream:
                            stream_started_successfully = True
                            is_first_chunk = True
                            async for chunk in response.content.iter_any():
                                # È¶ñÂ≠óÂõûË∞É TTFT
                                if is_first_chunk:
                                    ttft = time.perf_counter() - start_time
                                    self._trigger_stream_start_callback(req.on_stream_start, ctx.request_id, ttft, req.stream_start_data)
                                    is_first_chunk = False

                                # [‰øÆÊîπ] ‰ªÖÂΩìÊ∂àË¥πËÄÖÊú™Êñ≠ÂºÄÊó∂Êâç push Âà∞ÈòüÂàó
                                # Â¶ÇÊûúÂ∑≤Êñ≠ÂºÄÔºåË∑≥Ëøá push ‰ª•ÂÖçÈòªÂ°ûÔºå‰ΩÜÁªßÁª≠ÊâßË°å‰∏ãÈù¢ÁöÑ accumulate ÈÄªËæë
                                if not ctx.consumer_disconnected:
                                    await ctx.stream_queue.put(chunk)
                                
                                # [‰øÆÊîπ] Âç≥‰ΩøÊñ≠ÂºÄÔºå‰æùÁÑ∂ÁªüËÆ°ÂÆåÊï¥Êï∞ÊçÆ
                                if req.keep_content_in_memory: 
                                    accumulated_body.extend(chunk)
                                    if len(accumulated_body) > req.max_memory_size:
                                        raise ValueError(f"Response too large > {req.max_memory_size} bytes")

                            # ÊµÅÁªìÊùüÔºåÊîæÂÖ• sentinel (‰ªÖÂΩìÊ≤°Êñ≠ÂºÄÊó∂)
                            if not ctx.consumer_disconnected:
                                await ctx.stream_queue.put(ctx.sentinel)
                        else:
                            if req.keep_content_in_memory:
                                body_bytes = await response.read()
                                accumulated_body.extend(body_bytes)
                            else: await response.read()

                        ctx.status = RequestStatus.COMPLETED
                        final_content = self._prepare_content(accumulated_body, req.keep_content_in_memory)
                        parsed_json = self._try_parse_json(accumulated_body) if req.keep_content_in_memory else None
                        
                        result_obj = RequestResult(ctx.request_id, RequestStatus.COMPLETED, final_content, response.status, None, parsed_json)
                        if not ctx.result_future.done(): ctx.result_future.set_result(result_obj)
                        
                        self._trigger_callback(req.on_success, result_obj, req.user_data)
                        return 

                except asyncio.CancelledError as e:
                    ctx.status = RequestStatus.CANCELLED
                    partial = self._prepare_content(accumulated_body, req.keep_content_in_memory)
                    parsed_json = self._try_parse_json(accumulated_body) if req.keep_content_in_memory else None
                    
                    if req.is_stream and not ctx.consumer_disconnected: await ctx.stream_queue.put(e)
                    if not ctx.startup_future.done(): ctx.startup_future.set_exception(e)
                    
                    result_obj = RequestResult(ctx.request_id, RequestStatus.CANCELLED, partial, last_http_code, e, parsed_json)
                    if not ctx.result_future.done(): ctx.result_future.set_result(result_obj)

                    if req.cancel_behavior == CancelBehavior.TRIGGER_SUCCESS: self._trigger_callback(req.on_success, result_obj, req.user_data)
                    elif req.cancel_behavior == CancelBehavior.TRIGGER_FAILURE: self._trigger_callback(req.on_failure, result_obj, req.user_data)
                    raise

                except Exception as e:
                    last_error = e
                    if isinstance(e, HttpErrorWithContent): last_http_code = e.status_code
                    
                    if req.is_stream and stream_started_successfully and not req.retry_on_stream_error:
                        if not ctx.startup_future.done(): ctx.startup_future.set_exception(e)
                        break 
                    
                    # Ê£ÄÊü•ÊòØÂê¶ËøòÊúâÈáçËØïÊú∫‰ºö ‰∏î Êó∂Èó¥Ë∂≥Â§ü
                    if attempt < total_attempts and (deadline - time.time() > 0):
                        backoff = min(req.retry_interval * (2 ** (attempt - 1)), 10.0)
                        logger.warning(f"Retry {attempt} failed: {e}. Sleeping {backoff:.2f}s")
                        await asyncio.sleep(backoff)
                        continue
                    else:
                        if not ctx.startup_future.done(): ctx.startup_future.set_exception(e)
                        break
            
            ctx.status = RequestStatus.FAILED
            if req.is_stream and not ctx.consumer_disconnected:
                error_to_propagate = last_error if last_error else Exception("Unknown Error in Worker")
                await ctx.stream_queue.put(error_to_propagate)
            
            if not ctx.startup_future.done():
                ctx.startup_future.set_exception(last_error or Exception("Worker Failed"))
            
            final_content = self._prepare_content(accumulated_body, req.keep_content_in_memory)
            parsed_json = self._try_parse_json(accumulated_body) if req.keep_content_in_memory else None
            
            result_obj = RequestResult(ctx.request_id, RequestStatus.FAILED, final_content, last_http_code, last_error, parsed_json)
            
            if not ctx.result_future.done():
                ctx.result_future.set_exception(last_error or Exception("Request Failed"))

            self._trigger_callback(req.on_failure, result_obj, req.user_data)

        finally:
            # TTL GC ÂÖ≥ÈîÆÁÇπÔºöÊ†áËÆ∞ÁªìÊùüÊó∂Èó¥
            if ctx.finished_at is None:
                ctx.finished_at = datetime.now()

    def _prepare_content(self, data: bytearray, keep_memory: bool) -> Union[str, bytes]:
        if not keep_memory: return "[Content Dropped]"
        try: return data.decode('utf-8')
        except: return bytes(data)

    def _trigger_stream_start_callback(self, callback:Union[OnStreamStartCallback, None], req_id:str, ttft:float, custom_data:Any):
        if not callback: return
        if asyncio.iscoroutinefunction(callback): asyncio.create_task(callback(req_id, ttft, custom_data))
        else: asyncio.get_running_loop().run_in_executor(None, lambda: callback(req_id, ttft, custom_data))

    def _trigger_callback(self, callback:Union[CallbackType, None], result:RequestResult, user_data:Any):
        if not callback: return
        if asyncio.iscoroutinefunction(callback): asyncio.create_task(callback(result, user_data))
        else: asyncio.get_running_loop().run_in_executor(None, lambda: callback(result, user_data))

    async def cancel_request(self, request_id: str):
        ctx = self.active_requests.get(request_id)
        if ctx and ctx.task and not ctx.task.done():
            ctx.task.cancel()
            return True
        return False

    async def stream_generator(self, request_id: str) -> AsyncGenerator[bytes, None]:
        ctx = self.active_requests.get(request_id)
        if not ctx or not ctx.wrapper.is_stream: raise ValueError("Invalid ID")
        
        try:
            while True:
                data = await ctx.stream_queue.get()
                if isinstance(data, Exception): raise data
                if data is ctx.sentinel: break
                yield data
        finally:
            # [‰øÆÊîπ] Ê∂àË¥πËÄÖÊñ≠ÂºÄÈÄªËæë
            # ‰∏çÂÜç cancel workerÔºåËÄåÊòØÊ†áËÆ∞ disconnected Âπ∂Ê∏ÖÁ©∫ÈòüÂàó
            logger.info(f"‚ÑπÔ∏è Consumer disconnected for {request_id}. Worker will continue for stats.")
            ctx.consumer_disconnected = True
            
            # [ÂÖ≥ÈîÆ] ÂøÖÈ°ªÊ∏ÖÁ©∫ÈòüÂàóÔºå‰ª•Èò≤ worker Ê≠£Â•ΩÂç°Âú® queue.put() ‰∏äÁ≠âÂæÖÁ©∫‰Ωç
            # worker Âú®‰∏ã‰∏ÄËΩÆÂæ™ÁéØ‰ºöÊ£ÄÊü• consumer_disconnected Âπ∂ÂÅúÊ≠¢ put
            while not ctx.stream_queue.empty():
                try: ctx.stream_queue.get_nowait()
                except: break

    async def _garbage_collector(self):
        now = datetime.now()
        retention = timedelta(seconds=self.result_retention_seconds)
        keys_to_remove = []
        for req_id, ctx in self.active_requests.items():
            if ctx.task.done() and ctx.finished_at and (now - ctx.finished_at > retention):
                keys_to_remove.append(req_id)
        for key in keys_to_remove:
            if key in self.active_requests: del self.active_requests[key]

    async def close(self):
        logger.info("Closing AsyncHttpClient, cancelling active requests...")
        pending_tasks = []
        for _, ctx in self.active_requests.items():
            if ctx.task and not ctx.task.done():
                ctx.task.cancel()
                pending_tasks.append(ctx.task)
        
        if pending_tasks:
            await asyncio.gather(*pending_tasks, return_exceptions=True)
        
        if self.session: await self.session.close()
        self.scheduler.shutdown()