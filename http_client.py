import asyncio
import aiohttp
import uuid
import logging
import orjson
import time
from enum import Enum
from typing import Callable, Optional, Dict, Any, AsyncGenerator, Union, Awaitable
from dataclasses import dataclass, field
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from datetime import datetime, timedelta

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- æšä¸¾ä¸æ•°æ®ç±» ---

class CancelBehavior(Enum):
    DO_NOTHING = 0
    TRIGGER_SUCCESS = 1
    TRIGGER_FAILURE = 2

class RequestStatus(Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

@dataclass
class RequestResult:
    request_id: str
    status: RequestStatus
    content: Union[str, bytes, None]
    http_code: Optional[int] = None
    error: Optional[Exception] = None
    json_body: Any = None 

# å›è°ƒå‡½æ•°ç±»å‹å®šä¹‰
OnStreamStartCallback = Callable[[str, float, Any], Union[None, Awaitable[None]]]
CallbackType = Callable[['RequestResult', Any], Union[None, Awaitable[None]]]

@dataclass
class RequestWrapper:
    """
    HTTP è¯·æ±‚å°è£…ç±»
    """
    url: str
    method: str = "GET"
    params: Optional[Dict[str, Any]] = None
    headers: Optional[Dict[str, str]] = None
    data: Any = None
    json: Any = None
    
    # å›è°ƒç›¸å…³
    user_data: Any = None 
    on_stream_start: Optional[OnStreamStartCallback] = None
    stream_start_data: Any = None 
    on_success: Optional[CallbackType] = None
    on_failure: Optional[CallbackType] = None
    
    # è¡Œä¸ºé…ç½®
    keep_content_in_memory: bool = True 
    max_memory_size: int = 100 * 1024 * 1024 # 100 MB

    max_retries: int = 0
    retry_interval: float = 1.0
    is_stream: bool = False
    retry_on_stream_error: bool = True 
    timeout: int = 60
    cancel_behavior: CancelBehavior = CancelBehavior.TRIGGER_FAILURE
    
    extra_options: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self): 
        self.method = self.method.upper()

class HttpErrorWithContent(Exception):
    """æºå¸¦ä¸Šæ¸¸è¿”å›çš„é”™è¯¯ Body çš„å¼‚å¸¸"""
    def __init__(self, status_code: int = 500, content: Union[str, bytes, None] = None, message: str = ""):
        super().__init__(message)
        self.status_code = status_code
        self.content = content

class RequestContext:
    def __init__(self, request_id: str, wrapper: RequestWrapper):
        self.request_id = request_id
        self.wrapper = wrapper
        self.task: Optional[asyncio.Task] = None
        self.status = RequestStatus.PENDING
        self.created_at = datetime.now()
        self.finished_at: Optional[datetime] = None # ç”¨äº TTL GC
        
        self.stream_queue: Optional[asyncio.Queue] = asyncio.Queue(maxsize=1000) if wrapper.is_stream else None

        # [æ–°å¢] æ ‡è®°æ¶ˆè´¹è€…æ˜¯å¦å·²æ–­å¼€
        self.consumer_disconnected: bool = False

        self.sentinel = object()
        
        # ä¸¤ä¸ª Future ç”¨äºåŒæ­¥çŠ¶æ€
        self.startup_future: asyncio.Future = asyncio.Future() # è¿æ¥å»ºç«‹ï¼ˆHeaderæ¥æ”¶ï¼‰
        self.result_future: asyncio.Future = asyncio.Future()  # è¯·æ±‚å½»åº•å®Œæˆ

# --- æ ¸å¿ƒå®¢æˆ·ç«¯ ---

class AsyncHttpClient:
    def __init__(self, result_retention_seconds: int = 300):
        self.active_requests: Dict[str, RequestContext] = {}
        self.session: Optional[aiohttp.ClientSession] = None
        self.result_retention_seconds = result_retention_seconds
        
        self.scheduler = AsyncIOScheduler()
        self.scheduler.add_job(self._garbage_collector, 'interval', seconds=60)
        self.scheduler.start()

    async def get_session(self) -> aiohttp.ClientSession:
        if self.session is None or self.session.closed:
            # ä¸è®¾ç½® json_serializeï¼Œæˆ‘ä»¬åœ¨ _worker ä¸­æ‰‹åŠ¨ç”¨ orjson
            # å–æ¶ˆè¿æ¥æ•°é™åˆ¶ï¼šlimit=0 è¡¨ç¤ºæ— é™åˆ¶
            connector = aiohttp.TCPConnector(limit=2000, limit_per_host=2000)
            self.session = aiohttp.ClientSession(connector=connector)
        return self.session

    def submit(self, request: RequestWrapper) -> str:
        req_id = str(uuid.uuid4())
        ctx = RequestContext(req_id, request)
        task = asyncio.create_task(self._worker(ctx))
        ctx.task = task
        self.active_requests[req_id] = ctx
        return req_id
    
    def is_alive(self, request_id: str) -> bool:
        """
        æ£€æŸ¥è¯·æ±‚æ˜¯å¦ä¾ç„¶æ´»è·ƒï¼ˆPending æˆ– Runningï¼‰ã€‚
        å¦‚æœè¯·æ±‚å·²å®Œæˆã€å¤±è´¥ã€å–æ¶ˆæˆ–å·²è¢« GC æ¸…ç†ï¼Œè¿”å› Falseã€‚
        """
        ctx = self.active_requests.get(request_id)
        
        # 1. å¦‚æœä¸Šä¸‹æ–‡ä¸å­˜åœ¨ï¼ˆå·²è¢« GC æ¸…ç†ï¼‰ï¼Œåˆ™ä¸æ´»è·ƒ
        if not ctx:
            return False
        
        # 2. å¦‚æœä»»åŠ¡è¿˜æ²¡åˆ›å»ºï¼ˆå¤„äº Pendingï¼‰ï¼Œè§†ä¸ºæ´»è·ƒ
        if ctx.task is None:
            return True
        
        # 3. æ£€æŸ¥ asyncio.Task æ˜¯å¦å®Œæˆ
        if ctx.task.done():
            return False
            
        return True

    async def wait_for_upstream_status(self, request_id: str):
        """ç­‰å¾…ä¸Šæ¸¸è¿æ¥å»ºç«‹ç»“æœï¼Œé‡åˆ° 400/500 ä¼šæŠ›å‡ºå¼‚å¸¸"""
        ctx = self.active_requests.get(request_id)
        if not ctx: raise ValueError("Invalid Request ID")
        await ctx.startup_future

    async def _get_final_result(self, request_id: str) -> RequestResult:
        ctx = self.active_requests.get(request_id)
        if not ctx: raise ValueError(f"Request {request_id} not found or cleaned up")
        return await ctx.result_future

    async def content(self, request_id: str) -> bytes:
        res = await self._get_final_result(request_id)
        if isinstance(res.content, str): return res.content.encode('utf-8')
        return res.content or b""

    async def text(self, request_id: str, encoding: str = 'utf-8', errors: str = 'replace') -> str:
        res = await self._get_final_result(request_id)
        if isinstance(res.content, str): return res.content
        if isinstance(res.content, bytes):
            return res.content.decode(encoding, errors)
        if res.content is None:
            return ""
        return res.content.decode(encoding, errors) if res.content else ""

    async def json(self, request_id: str) -> Any:
        res = await self._get_final_result(request_id)
        if res.json_body is not None: return res.json_body
        if res.content: return orjson.loads(res.content)
        return None

    def _try_parse_json(self, data: bytearray) -> Any:
        if not data: return None
        try: return orjson.loads(bytes(data))
        except Exception: return None

    async def _worker(self, ctx: RequestContext):
        req = ctx.wrapper
        ctx.status = RequestStatus.RUNNING
        session = await self.get_session()
        deadline = time.time() + req.timeout
        
        request_kwargs = {
            'params': req.params, 'headers': req.headers or {}, **req.extra_options
        }
        if req.json is not None:
            request_kwargs['data'] = orjson.dumps(req.json)
            if 'headers' not in request_kwargs: request_kwargs['headers'] = {}
            request_kwargs['headers']['Content-Type'] = 'application/json'
        elif req.data is not None:
            request_kwargs['data'] = req.data

        total_attempts = req.max_retries + 1
        accumulated_body = bytearray()
        last_http_code, last_error = None, None
        
        # è®¤è¯é”™è¯¯çš„é‡è¯•è®¡æ•°ï¼ˆ401, 403, 521, 523ï¼‰æœ€å¤šé‡è¯•2æ¬¡
        auth_error_retry_count = 0
        # å®šä¹‰é”™è¯¯ç é›†åˆ
        auth_errors = {401, 403, 521, 523}
        normal_retry_errors = {408, 429, 470, 471, 472, 500, 502, 503, 504, 520, 522, 524}
        
        try:
            for attempt in range(1, total_attempts + 1):
                remaining_time = deadline - time.time()
                if remaining_time <= 0:
                    last_error = asyncio.TimeoutError("Global timeout exceeded")
                    logger.error(f"âŒ Global timeout exceeded for {ctx.request_id}")
                    break
                request_kwargs['timeout'] = aiohttp.ClientTimeout(
                    total=remaining_time,
                    connect=35,
                    sock_connect=30,
                )
                accumulated_body = bytearray()
                last_http_code, last_error = None, None
                stream_started_successfully = False
                start_time = time.perf_counter()

                try:
                    if attempt > 1:
                        logger.info(f"ğŸ”„ Retry attempt {attempt}/{total_attempts} for {ctx.request_id}")

                    async with session.request(req.method, req.url, **request_kwargs) as response:
                        last_http_code = response.status
                        
                        # --- é”™è¯¯çŠ¶æ€ç å¤„ç† ---
                        if response.status >= 400:
                            try:
                                error_bytes = await response.read()
                                if req.keep_content_in_memory: accumulated_body.extend(error_bytes)
                            except: error_bytes = b""
                            
                            # æŠ›å‡ºå¼‚å¸¸è¿›å…¥ except å—å¤„ç†ï¼ˆå†³å®šæ˜¯å¦é‡è¯•ï¼‰
                            raise HttpErrorWithContent(response.status, error_bytes, f"HTTP {response.status}")

                        # --- è¿æ¥æˆåŠŸï¼Œé€šçŸ¥ wait_for_upstream_status ---
                        if not ctx.startup_future.done():
                            ctx.startup_future.set_result(response.status)

                        if req.is_stream:
                            stream_started_successfully = True
                            is_first_chunk = True
                            async for chunk in response.content:
                                # é¦–å­—å›è°ƒ TTFT
                                if is_first_chunk:
                                    ttft = time.perf_counter() - start_time
                                    self._trigger_stream_start_callback(req.on_stream_start, ctx.request_id, ttft, req.stream_start_data)
                                    is_first_chunk = False

                                # [ä¿®æ”¹] ä»…å½“æ¶ˆè´¹è€…æœªæ–­å¼€æ—¶æ‰ push åˆ°é˜Ÿåˆ—
                                # å¦‚æœå·²æ–­å¼€ï¼Œè·³è¿‡ push ä»¥å…é˜»å¡ï¼Œä½†ç»§ç»­æ‰§è¡Œä¸‹é¢çš„ accumulate é€»è¾‘
                                if not ctx.consumer_disconnected:
                                    await ctx.stream_queue.put(chunk)
                                
                                # [ä¿®æ”¹] å³ä½¿æ–­å¼€ï¼Œä¾ç„¶ç»Ÿè®¡å®Œæ•´æ•°æ®
                                if req.keep_content_in_memory: 
                                    accumulated_body.extend(chunk)
                                    if len(accumulated_body) > req.max_memory_size:
                                        raise ValueError(f"Response too large > {req.max_memory_size} bytes")

                            # æµç»“æŸï¼Œæ”¾å…¥ sentinel (ä»…å½“æ²¡æ–­å¼€æ—¶)
                            if not ctx.consumer_disconnected:
                                await ctx.stream_queue.put(ctx.sentinel)
                        else:
                            if req.keep_content_in_memory:
                                body_bytes = await response.read()
                                accumulated_body.extend(body_bytes)
                            else: await response.read()

                        ctx.status = RequestStatus.COMPLETED
                        final_content = self._prepare_content(accumulated_body, req.keep_content_in_memory)
                        parsed_json = self._try_parse_json(accumulated_body) if req.keep_content_in_memory else None
                        
                        result_obj = RequestResult(ctx.request_id, RequestStatus.COMPLETED, final_content, response.status, None, parsed_json)
                        if not ctx.result_future.done(): ctx.result_future.set_result(result_obj)
                        
                        self._trigger_callback(req.on_success, result_obj, req.user_data)
                        return 

                except asyncio.CancelledError as e:
                    ctx.status = RequestStatus.CANCELLED
                    partial = self._prepare_content(accumulated_body, req.keep_content_in_memory)
                    parsed_json = self._try_parse_json(accumulated_body) if req.keep_content_in_memory else None
                    
                    if req.is_stream and not ctx.consumer_disconnected: await ctx.stream_queue.put(e)
                    if not ctx.startup_future.done(): ctx.startup_future.set_exception(e)
                    
                    result_obj = RequestResult(ctx.request_id, RequestStatus.CANCELLED, partial, last_http_code, e, parsed_json)
                    if not ctx.result_future.done(): ctx.result_future.set_result(result_obj)

                    if req.cancel_behavior == CancelBehavior.TRIGGER_SUCCESS: self._trigger_callback(req.on_success, result_obj, req.user_data)
                    elif req.cancel_behavior == CancelBehavior.TRIGGER_FAILURE: self._trigger_callback(req.on_failure, result_obj, req.user_data)
                    raise

                except Exception as e:
                    last_error = e
                    should_stop_retry = False
                    is_auth_error = False
                    is_normal_retry_error = False

                    if isinstance(e, HttpErrorWithContent): 
                        last_http_code = e.status_code
                        is_auth_error = e.status_code in auth_errors
                        is_normal_retry_error = e.status_code in normal_retry_errors
                        
                        # å¦‚æœä¸æ˜¯å¯é‡è¯•çš„é”™è¯¯ç ï¼Œç›´æ¥åœæ­¢
                        if not is_auth_error and not is_normal_retry_error:
                            should_stop_retry = True
                    
                    if req.is_stream and stream_started_successfully and not req.retry_on_stream_error:
                        should_stop_retry = True

                    if should_stop_retry:
                        if not ctx.startup_future.done(): ctx.startup_future.set_exception(e)
                        break 
                    
                    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰é‡è¯•æœºä¼š ä¸” æ—¶é—´è¶³å¤Ÿ
                    if attempt < total_attempts and (deadline - time.time() > 0):
                        # é’ˆå¯¹è®¤è¯é”™è¯¯çš„ç‰¹æ®Šå¤„ç†
                        if is_auth_error:
                            if auth_error_retry_count >= 2:
                                # è®¤è¯é”™è¯¯å·²é‡è¯•2æ¬¡ï¼Œä¸å†é‡è¯•
                                logger.warning(f"Auth error retry limit reached (2 times) for {ctx.request_id}")
                                if not ctx.startup_future.done(): ctx.startup_future.set_exception(e)
                                break
                            # è®¤è¯é”™è¯¯ä½¿ç”¨å›ºå®šé—´éš”ï¼Œä¸é€€å¡
                            backoff = req.retry_interval
                            auth_error_retry_count += 1
                            logger.warning(f"Auth error retry {auth_error_retry_count}/2 for {ctx.request_id}. Sleeping {backoff:.2f}s (fixed)")
                        elif is_normal_retry_error:
                            # æ™®é€šå¯é‡è¯•é”™è¯¯ä½¿ç”¨æŒ‡æ•°é€€é¿
                            backoff = min(req.retry_interval * (2 ** (attempt - 1)), 10.0)
                            logger.warning(f"Retry {attempt}/{total_attempts} for {ctx.request_id}. Sleeping {backoff:.2f}s (exponential backoff)")
                        else:
                            # å…¶ä»–é”™è¯¯ä¹Ÿä½¿ç”¨æŒ‡æ•°é€€é¿
                            backoff = min(req.retry_interval * (2 ** (attempt - 1)), 10.0)
                            logger.warning(f"Retry {attempt}/{total_attempts} for {ctx.request_id}. Sleeping {backoff:.2f}s")
                        
                        await asyncio.sleep(backoff)
                        continue
                    else:
                        if not ctx.startup_future.done(): ctx.startup_future.set_exception(e)
                        break
            
            ctx.status = RequestStatus.FAILED
            if req.is_stream and not ctx.consumer_disconnected:
                error_to_propagate = last_error if last_error else Exception("Unknown Error in Worker")
                await ctx.stream_queue.put(error_to_propagate)
            
            if not ctx.startup_future.done():
                ctx.startup_future.set_exception(last_error or Exception("Worker Failed"))
            
            final_content = self._prepare_content(accumulated_body, req.keep_content_in_memory)
            parsed_json = self._try_parse_json(accumulated_body) if req.keep_content_in_memory else None
            
            result_obj = RequestResult(ctx.request_id, RequestStatus.FAILED, final_content, last_http_code, last_error, parsed_json)
            
            if not ctx.result_future.done():
                ctx.result_future.set_exception(last_error or Exception("Request Failed"))

            self._trigger_callback(req.on_failure, result_obj, req.user_data)

        finally:
            # TTL GC å…³é”®ç‚¹ï¼šæ ‡è®°ç»“æŸæ—¶é—´
            if ctx.finished_at is None:
                ctx.finished_at = datetime.now()

    def _prepare_content(self, data: bytearray, keep_memory: bool) -> Union[str, bytes]:
        if not keep_memory: return "[Content Dropped]"
        try: return data.decode('utf-8')
        except: return bytes(data)

    def _trigger_stream_start_callback(self, callback:Union[OnStreamStartCallback, None], req_id:str, ttft:float, custom_data:Any):
        if not callback: return
        if asyncio.iscoroutinefunction(callback): asyncio.create_task(callback(req_id, ttft, custom_data))
        else: asyncio.get_running_loop().run_in_executor(None, lambda: callback(req_id, ttft, custom_data))

    def _trigger_callback(self, callback:Union[CallbackType, None], result:RequestResult, user_data:Any):
        if not callback: return
        if asyncio.iscoroutinefunction(callback): asyncio.create_task(callback(result, user_data))
        else: asyncio.get_running_loop().run_in_executor(None, lambda: callback(result, user_data))

    async def cancel_request(self, request_id: str):
        ctx = self.active_requests.get(request_id)
        if ctx and ctx.task and not ctx.task.done():
            ctx.task.cancel()
            return True
        return False

    async def stream_generator(self, request_id: str) -> AsyncGenerator[bytes, None]:
        ctx = self.active_requests.get(request_id)
        if not ctx or not ctx.wrapper.is_stream: raise ValueError("Invalid ID")
        
        try:
            while True:
                data = await ctx.stream_queue.get()
                if isinstance(data, Exception): raise data
                if data is ctx.sentinel: break
                yield data
        finally:
            # [ä¿®æ”¹] æ¶ˆè´¹è€…æ–­å¼€é€»è¾‘
            # ä¸å† cancel workerï¼Œè€Œæ˜¯æ ‡è®° disconnected å¹¶æ¸…ç©ºé˜Ÿåˆ—
            logger.info(f"â„¹ï¸ Consumer disconnected for {request_id}. Worker will continue for stats.")
            ctx.consumer_disconnected = True
            
            # [å…³é”®] å¿…é¡»æ¸…ç©ºé˜Ÿåˆ—ï¼Œä»¥é˜² worker æ­£å¥½å¡åœ¨ queue.put() ä¸Šç­‰å¾…ç©ºä½
            # worker åœ¨ä¸‹ä¸€è½®å¾ªç¯ä¼šæ£€æŸ¥ consumer_disconnected å¹¶åœæ­¢ put
            while not ctx.stream_queue.empty():
                try: ctx.stream_queue.get_nowait()
                except: break

    async def _garbage_collector(self):
        now = datetime.now()
        retention = timedelta(seconds=self.result_retention_seconds)
        keys_to_remove = []
        for req_id, ctx in self.active_requests.items():
            if ctx.task.done() and ctx.finished_at and (now - ctx.finished_at > retention):
                keys_to_remove.append(req_id)
        for key in keys_to_remove:
            if key in self.active_requests: del self.active_requests[key]

    async def close(self):
        logger.info("Closing AsyncHttpClient, cancelling active requests...")
        pending_tasks = []
        for _, ctx in self.active_requests.items():
            if ctx.task and not ctx.task.done():
                ctx.task.cancel()
                pending_tasks.append(ctx.task)
        
        if pending_tasks:
            await asyncio.gather(*pending_tasks, return_exceptions=True)
        
        if self.session: await self.session.close()
        self.scheduler.shutdown()